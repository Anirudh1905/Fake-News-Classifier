{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ajS_9ZRCiY2cKFAJireVNBT-PIwdW8JN",
      "authorship_tag": "ABX9TyM6dCVWQNBCpwnAofvOaa6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anirudh1905/Fake-News-Classifier/blob/master/Fake_News_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZThChxEV4RG",
        "colab_type": "text"
      },
      "source": [
        "# Fake News Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeR9bo9QV4oL",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgGiVYcn8OCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3VAQFSLJGGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f7425587-53dc-436e-fbf1-25ea5e155bc6"
      },
      "source": [
        "df=pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BejXmcHVJKUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGieLBFqJR00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=df.drop('label',axis=1)\n",
        "y=df['label']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLuFYQDiJbTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75bad7ac-86ae-4591-c035-c61e4bd4c8a3"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18285, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5JLMqqnJczY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5ff06ce-034d-4f87-b2c4-b57f43b41e45"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18285,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOMa9DgQJheU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import one_hot"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq-3aNsXKQ8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size=10000"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83tbuz-RKT4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages=x.copy()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZfYGh2NKYVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "03393d29-d1df-4d61-81c5-2cfe4d1803ef"
      },
      "source": [
        "messages['text'][1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed “no debate so far as I could ascertain as to who their spokesman was to be” (kind of the like the Democratic primaries in 2016 minus the   terms unknown then even at a Seven Sisters school). “I am very glad that Miss Adams made it clear that what I am speaking for today is all of us —  the 400 of us,” Miss Rodham told her classmates. After appointing herself Edger Bergen to the Charlie McCarthys and Mortimer Snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom —  or at least John Lennon wisdom) took issue with the previous speaker. Despite becoming the first   to win election to a seat in the U. S. Senate since Reconstruction, Edward Brooke came in for criticism for calling for “empathy” for the goals of protestors as he criticized tactics. Though Clinton in her senior thesis on Saul Alinsky lamented “Black Power demagogues” and “elitist arrogance and repressive intolerance” within the New Left, similar words coming out of a Republican necessitated a brief rebuttal. “Trust,” Rodham ironically observed in 1969, “this is one word that when I asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said ‘Talk about trust, talk about the lack of trust both for us and the way we feel about others. Talk about the trust bust.’ What can you say about it? What can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?” The “trust bust” certainly busted Clinton’s 2016 plans. She certainly did not even understand that people distrusted her. After Whitewater, Travelgate, the vast   conspiracy, Benghazi, and the missing emails, Clinton found herself the distrusted voice on Friday. There was a load of compromising on the road to the broadening of her political horizons. And distrust from the American people —  Trump edged her 48 percent to 38 percent on the question immediately prior to November’s election —  stood as a major reason for the closing of those horizons. Clinton described her vanquisher and his supporters as embracing a “lie,” a “con,” “alternative facts,” and “a   assault on truth and reason. ” She failed to explain why the American people chose his lies over her truth. “As the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,” she offered. “That is not hyperbole. ” Like so many people to emerge from the 1960s, Hillary Clinton embarked upon a long, strange trip. From high school Goldwater Girl and Wellesley College Republican president to Democratic politician, Clinton drank in the times and the place that gave her a degree. More significantly, she went from idealist to cynic, as a comparison of her two Wellesley commencement addresses show. Way back when, she lamented that “for too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. ” Now, as the big woman on campus but the odd woman out of the White House, she wonders how her current station is even possible. “Why aren’t I 50 points ahead?” she asked in September. In May she asks why she isn’t president. The woman famously dubbed a “congenital liar” by Bill Safire concludes that lies did her in —  theirs, mind you, not hers. Getting stood up on Election Day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUGll-SSKkbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "messages.reset_index(inplace=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgVTVV0ZKma-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j03laEwILJZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d5e3c29e-6c5e-4bb8-df00-925316853fb7"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9srRLjMWuyL",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEaKlUZ1LNEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lm=WordNetLemmatizer()\n",
        "corpus=[]\n",
        "for i in range(0,len(messages)):\n",
        "  rv=re.sub('[^a-zA-Z]',' ',messages['text'][i])\n",
        "  rv=rv.lower()\n",
        "  rv=rv.split()\n",
        "  rv=[lm.lemmatize(word) for word in rv if not word in stopwords.words('english')]\n",
        "  rv=' '.join(rv)\n",
        "  corpus.append(rv)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9eQoJ1nMaCX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "c3979e01-26b9-4489-a4f3-32d99e2d1dae"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'house dem aide even see comey letter jason chaffetz tweeted darrell lucus october subscribe jason chaffetz stump american fork utah image courtesy michael jolley available creative common license apology keith olbermann doubt worst person world week fbi director james comey according house democratic aide look like also know second worst person well turn comey sent infamous letter announcing fbi looking email may related hillary clinton email server ranking democrat relevant committee hear comey found via tweet one republican committee chairman know comey notified republican chairman democratic ranking member house intelligence judiciary oversight committee agency reviewing email recently discovered order see contained classified information long letter went oversight committee chairman jason chaffetz set political world ablaze tweet fbi dir informed fbi learned existence email appear pertinent investigation case reopened jason chaffetz jasoninthehouse october course know case comey actually saying reviewing email light unrelated case know anthony weiner sexting teenager apparently little thing fact matter chaffetz utah republican already vowed initiate raft investigation hillary win least two year worth possibly entire term worth apparently chaffetz thought fbi already work resulting tweet briefly roiled nation cooler head realized dud according senior house democratic aide misreading letter may least chaffetz sin aide told shareblue bos democrat even know comey letter time found checked twitter democratic ranking member relevant committee receive comey letter republican chairman fact democratic ranking member receive chairman oversight government reform committee jason chaffetz tweeted made public let see got right fbi director tell chaffetz gop committee chairman major development potentially politically explosive investigation neither chaffetz colleague courtesy let democratic counterpart know instead according aide made find twitter already talk daily ko comey provided advance notice letter chaffetz republican giving time turn spin machine may make good theater nothing far even suggests case nothing far suggests comey anything grossly incompetent tone deaf suggest however chaffetz acting way make dan burton darrell issa look like model responsibility bipartisanship even decency notify ranking member elijah cummings something explosive trample basic standard fairness know granted likely chaffetz answer sits ridiculously republican district anchored provo orem cook partisan voting index r gave mitt romney punishing percent vote moreover republican house leadership given full support chaffetz planned fishing expedition mean turn hot light textbook example house become republican control also second worst person world darrell lucus darrell something graduate university north carolina considers journalist old school attempt turn member religious right college succeeded turning religious right worst nightmare charismatic christian unapologetic liberal desire stand scared silence increased survived abusive three year marriage may know daily ko christian dem nc follow twitter darrelllucus connect facebook click buy darrell mello yello connect'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMDoaW0YSBi0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "127134fe-dfac-42dd-c86c-6dd87691ff87"
      },
      "source": [
        "one_hot_rp=[one_hot(words,vocab_size)for words in corpus]\n",
        "one_hot_rp[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5836,\n",
              " 9284,\n",
              " 6804,\n",
              " 2141,\n",
              " 4486,\n",
              " 6432,\n",
              " 1813,\n",
              " 7310,\n",
              " 536,\n",
              " 1503,\n",
              " 5948,\n",
              " 1604,\n",
              " 9890,\n",
              " 419,\n",
              " 7310,\n",
              " 536,\n",
              " 2328,\n",
              " 3674,\n",
              " 2261,\n",
              " 9243,\n",
              " 7303,\n",
              " 7176,\n",
              " 6857,\n",
              " 4717,\n",
              " 6309,\n",
              " 2850,\n",
              " 3029,\n",
              " 2305,\n",
              " 2540,\n",
              " 3933,\n",
              " 291,\n",
              " 910,\n",
              " 8620,\n",
              " 4308,\n",
              " 7316,\n",
              " 6199,\n",
              " 4963,\n",
              " 6768,\n",
              " 9530,\n",
              " 6432,\n",
              " 5652,\n",
              " 5836,\n",
              " 1689,\n",
              " 6804,\n",
              " 1053,\n",
              " 79,\n",
              " 3447,\n",
              " 3693,\n",
              " 1323,\n",
              " 8620,\n",
              " 4308,\n",
              " 3190,\n",
              " 990,\n",
              " 6432,\n",
              " 1922,\n",
              " 371,\n",
              " 1813,\n",
              " 3371,\n",
              " 4963,\n",
              " 4217,\n",
              " 117,\n",
              " 2122,\n",
              " 3262,\n",
              " 7599,\n",
              " 4954,\n",
              " 117,\n",
              " 5658,\n",
              " 3016,\n",
              " 5873,\n",
              " 9384,\n",
              " 6434,\n",
              " 3008,\n",
              " 6432,\n",
              " 3826,\n",
              " 1017,\n",
              " 8250,\n",
              " 62,\n",
              " 4641,\n",
              " 6434,\n",
              " 8433,\n",
              " 3693,\n",
              " 6432,\n",
              " 2478,\n",
              " 4641,\n",
              " 8433,\n",
              " 1689,\n",
              " 3016,\n",
              " 4721,\n",
              " 5836,\n",
              " 6678,\n",
              " 4619,\n",
              " 3158,\n",
              " 6434,\n",
              " 8639,\n",
              " 9535,\n",
              " 117,\n",
              " 9363,\n",
              " 717,\n",
              " 1026,\n",
              " 4486,\n",
              " 5175,\n",
              " 6682,\n",
              " 323,\n",
              " 7046,\n",
              " 1813,\n",
              " 2543,\n",
              " 3158,\n",
              " 6434,\n",
              " 8433,\n",
              " 7310,\n",
              " 536,\n",
              " 9783,\n",
              " 4344,\n",
              " 7316,\n",
              " 8215,\n",
              " 8250,\n",
              " 4963,\n",
              " 8261,\n",
              " 4458,\n",
              " 4963,\n",
              " 3980,\n",
              " 9405,\n",
              " 117,\n",
              " 3838,\n",
              " 8545,\n",
              " 9606,\n",
              " 3574,\n",
              " 8830,\n",
              " 7310,\n",
              " 536,\n",
              " 4984,\n",
              " 9890,\n",
              " 3558,\n",
              " 3693,\n",
              " 3574,\n",
              " 6432,\n",
              " 541,\n",
              " 2320,\n",
              " 9535,\n",
              " 117,\n",
              " 2683,\n",
              " 1288,\n",
              " 3574,\n",
              " 3693,\n",
              " 5588,\n",
              " 8601,\n",
              " 6357,\n",
              " 7538,\n",
              " 8267,\n",
              " 9728,\n",
              " 302,\n",
              " 1071,\n",
              " 5806,\n",
              " 536,\n",
              " 9243,\n",
              " 4641,\n",
              " 6543,\n",
              " 1869,\n",
              " 3523,\n",
              " 9699,\n",
              " 9606,\n",
              " 7599,\n",
              " 7852,\n",
              " 4550,\n",
              " 7463,\n",
              " 8673,\n",
              " 9978,\n",
              " 9367,\n",
              " 6486,\n",
              " 5240,\n",
              " 9978,\n",
              " 8267,\n",
              " 536,\n",
              " 9092,\n",
              " 4963,\n",
              " 6543,\n",
              " 9348,\n",
              " 9663,\n",
              " 8250,\n",
              " 9419,\n",
              " 5263,\n",
              " 5178,\n",
              " 5003,\n",
              " 8942,\n",
              " 9615,\n",
              " 5063,\n",
              " 5652,\n",
              " 2453,\n",
              " 5836,\n",
              " 1689,\n",
              " 6804,\n",
              " 5092,\n",
              " 1813,\n",
              " 2122,\n",
              " 4550,\n",
              " 536,\n",
              " 4278,\n",
              " 6804,\n",
              " 4475,\n",
              " 3682,\n",
              " 8412,\n",
              " 5873,\n",
              " 2141,\n",
              " 3693,\n",
              " 6432,\n",
              " 1813,\n",
              " 4504,\n",
              " 3826,\n",
              " 5090,\n",
              " 9217,\n",
              " 1689,\n",
              " 3016,\n",
              " 4721,\n",
              " 9384,\n",
              " 6434,\n",
              " 1404,\n",
              " 6432,\n",
              " 1813,\n",
              " 4641,\n",
              " 8433,\n",
              " 1071,\n",
              " 1689,\n",
              " 3016,\n",
              " 4721,\n",
              " 1404,\n",
              " 8433,\n",
              " 3158,\n",
              " 5069,\n",
              " 9455,\n",
              " 6434,\n",
              " 7310,\n",
              " 536,\n",
              " 1503,\n",
              " 3856,\n",
              " 2879,\n",
              " 6766,\n",
              " 4486,\n",
              " 1695,\n",
              " 1841,\n",
              " 4963,\n",
              " 6768,\n",
              " 9034,\n",
              " 536,\n",
              " 622,\n",
              " 6434,\n",
              " 8433,\n",
              " 5499,\n",
              " 734,\n",
              " 8904,\n",
              " 9673,\n",
              " 5115,\n",
              " 9606,\n",
              " 1884,\n",
              " 536,\n",
              " 9102,\n",
              " 7176,\n",
              " 6766,\n",
              " 1689,\n",
              " 7932,\n",
              " 3693,\n",
              " 1579,\n",
              " 5652,\n",
              " 6804,\n",
              " 3856,\n",
              " 4139,\n",
              " 9217,\n",
              " 6543,\n",
              " 3674,\n",
              " 922,\n",
              " 496,\n",
              " 6432,\n",
              " 568,\n",
              " 4102,\n",
              " 9712,\n",
              " 1813,\n",
              " 536,\n",
              " 4641,\n",
              " 9149,\n",
              " 4504,\n",
              " 990,\n",
              " 101,\n",
              " 3550,\n",
              " 2122,\n",
              " 4000,\n",
              " 7953,\n",
              " 6008,\n",
              " 1212,\n",
              " 5475,\n",
              " 2141,\n",
              " 9492,\n",
              " 3574,\n",
              " 1212,\n",
              " 5475,\n",
              " 9492,\n",
              " 6432,\n",
              " 9074,\n",
              " 5513,\n",
              " 4024,\n",
              " 8579,\n",
              " 1520,\n",
              " 6675,\n",
              " 153,\n",
              " 536,\n",
              " 6651,\n",
              " 564,\n",
              " 4000,\n",
              " 5139,\n",
              " 6405,\n",
              " 5948,\n",
              " 7882,\n",
              " 1053,\n",
              " 79,\n",
              " 6661,\n",
              " 2482,\n",
              " 767,\n",
              " 2141,\n",
              " 84,\n",
              " 9010,\n",
              " 3016,\n",
              " 4721,\n",
              " 5575,\n",
              " 6700,\n",
              " 3877,\n",
              " 5115,\n",
              " 6121,\n",
              " 8355,\n",
              " 1120,\n",
              " 8981,\n",
              " 3693,\n",
              " 1546,\n",
              " 3674,\n",
              " 536,\n",
              " 4429,\n",
              " 3333,\n",
              " 5325,\n",
              " 4641,\n",
              " 7347,\n",
              " 8708,\n",
              " 2651,\n",
              " 3052,\n",
              " 8294,\n",
              " 7243,\n",
              " 7820,\n",
              " 3524,\n",
              " 9218,\n",
              " 6278,\n",
              " 937,\n",
              " 3849,\n",
              " 4229,\n",
              " 5095,\n",
              " 8310,\n",
              " 2830,\n",
              " 4641,\n",
              " 5836,\n",
              " 7258,\n",
              " 5399,\n",
              " 4543,\n",
              " 6006,\n",
              " 536,\n",
              " 6045,\n",
              " 9535,\n",
              " 1887,\n",
              " 1443,\n",
              " 990,\n",
              " 6104,\n",
              " 2683,\n",
              " 8906,\n",
              " 5711,\n",
              " 5836,\n",
              " 9003,\n",
              " 4641,\n",
              " 4476,\n",
              " 3447,\n",
              " 1323,\n",
              " 8620,\n",
              " 4308,\n",
              " 7316,\n",
              " 5948,\n",
              " 1604,\n",
              " 5948,\n",
              " 3877,\n",
              " 7853,\n",
              " 8645,\n",
              " 3834,\n",
              " 6071,\n",
              " 7331,\n",
              " 3631,\n",
              " 9511,\n",
              " 5256,\n",
              " 6843,\n",
              " 990,\n",
              " 4721,\n",
              " 1845,\n",
              " 1841,\n",
              " 7197,\n",
              " 5909,\n",
              " 2192,\n",
              " 1845,\n",
              " 1841,\n",
              " 8620,\n",
              " 74,\n",
              " 1981,\n",
              " 4610,\n",
              " 9036,\n",
              " 6171,\n",
              " 4833,\n",
              " 3029,\n",
              " 6246,\n",
              " 9315,\n",
              " 8688,\n",
              " 9046,\n",
              " 800,\n",
              " 3578,\n",
              " 8673,\n",
              " 9358,\n",
              " 2122,\n",
              " 3693,\n",
              " 922,\n",
              " 496,\n",
              " 4610,\n",
              " 9284,\n",
              " 8401,\n",
              " 7628,\n",
              " 9217,\n",
              " 2053,\n",
              " 7670,\n",
              " 781,\n",
              " 5128,\n",
              " 8316,\n",
              " 5948,\n",
              " 9636,\n",
              " 6788,\n",
              " 7670]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REJoPWSAWUIN",
        "colab_type": "text"
      },
      "source": [
        "## Embedding Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvaaoRYkSb5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f45f1444-6e09-46a9-d9cc-763f167e07cd"
      },
      "source": [
        "s_len=len(max(corpus))\n",
        "s_len"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R97CXuFtPmrX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "71d8d185-bb5d-46b0-928b-6f9138ab7e97"
      },
      "source": [
        "embedding_docs=pad_sequences(one_hot_rp,padding='pre',maxlen=s_len)\n",
        "embedding_docs[0]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0, 5836, 9284, 6804, 2141, 4486, 6432, 1813, 7310,\n",
              "        536, 1503, 5948, 1604, 9890,  419, 7310,  536, 2328, 3674, 2261,\n",
              "       9243, 7303, 7176, 6857, 4717, 6309, 2850, 3029, 2305, 2540, 3933,\n",
              "        291,  910, 8620, 4308, 7316, 6199, 4963, 6768, 9530, 6432, 5652,\n",
              "       5836, 1689, 6804, 1053,   79, 3447, 3693, 1323, 8620, 4308, 3190,\n",
              "        990, 6432, 1922,  371, 1813, 3371, 4963, 4217,  117, 2122, 3262,\n",
              "       7599, 4954,  117, 5658, 3016, 5873, 9384, 6434, 3008, 6432, 3826,\n",
              "       1017, 8250,   62, 4641, 6434, 8433, 3693, 6432, 2478, 4641, 8433,\n",
              "       1689, 3016, 4721, 5836, 6678, 4619, 3158, 6434, 8639, 9535,  117,\n",
              "       9363,  717, 1026, 4486, 5175, 6682,  323, 7046, 1813, 2543, 3158,\n",
              "       6434, 8433, 7310,  536, 9783, 4344, 7316, 8215, 8250, 4963, 8261,\n",
              "       4458, 4963, 3980, 9405,  117, 3838, 8545, 9606, 3574, 8830, 7310,\n",
              "        536, 4984, 9890, 3558, 3693, 3574, 6432,  541, 2320, 9535,  117,\n",
              "       2683, 1288, 3574, 3693, 5588, 8601, 6357, 7538, 8267, 9728,  302,\n",
              "       1071, 5806,  536, 9243, 4641, 6543, 1869, 3523, 9699, 9606, 7599,\n",
              "       7852, 4550, 7463, 8673, 9978, 9367, 6486, 5240, 9978, 8267,  536,\n",
              "       9092, 4963, 6543, 9348, 9663, 8250, 9419, 5263, 5178, 5003, 8942,\n",
              "       9615, 5063, 5652, 2453, 5836, 1689, 6804, 5092, 1813, 2122, 4550,\n",
              "        536, 4278, 6804, 4475, 3682, 8412, 5873, 2141, 3693, 6432, 1813,\n",
              "       4504, 3826, 5090, 9217, 1689, 3016, 4721, 9384, 6434, 1404, 6432,\n",
              "       1813, 4641, 8433, 1071, 1689, 3016, 4721, 1404, 8433, 3158, 5069,\n",
              "       9455, 6434, 7310,  536, 1503, 3856, 2879, 6766, 4486, 1695, 1841,\n",
              "       4963, 6768, 9034,  536,  622, 6434, 8433, 5499,  734, 8904, 9673,\n",
              "       5115, 9606, 1884,  536, 9102, 7176, 6766, 1689, 7932, 3693, 1579,\n",
              "       5652, 6804, 3856, 4139, 9217, 6543, 3674,  922,  496, 6432,  568,\n",
              "       4102, 9712, 1813,  536, 4641, 9149, 4504,  990,  101, 3550, 2122,\n",
              "       4000, 7953, 6008, 1212, 5475, 2141, 9492, 3574, 1212, 5475, 9492,\n",
              "       6432, 9074, 5513, 4024, 8579, 1520, 6675,  153,  536, 6651,  564,\n",
              "       4000, 5139, 6405, 5948, 7882, 1053,   79, 6661, 2482,  767, 2141,\n",
              "         84, 9010, 3016, 4721, 5575, 6700, 3877, 5115, 6121, 8355, 1120,\n",
              "       8981, 3693, 1546, 3674,  536, 4429, 3333, 5325, 4641, 7347, 8708,\n",
              "       2651, 3052, 8294, 7243, 7820, 3524, 9218, 6278,  937, 3849, 4229,\n",
              "       5095, 8310, 2830, 4641, 5836, 7258, 5399, 4543, 6006,  536, 6045,\n",
              "       9535, 1887, 1443,  990, 6104, 2683, 8906, 5711, 5836, 9003, 4641,\n",
              "       4476, 3447, 1323, 8620, 4308, 7316, 5948, 1604, 5948, 3877, 7853,\n",
              "       8645, 3834, 6071, 7331, 3631, 9511, 5256, 6843,  990, 4721, 1845,\n",
              "       1841, 7197, 5909, 2192, 1845, 1841, 8620,   74, 1981, 4610, 9036,\n",
              "       6171, 4833, 3029, 6246, 9315, 8688, 9046,  800, 3578, 8673, 9358,\n",
              "       2122, 3693,  922,  496, 4610, 9284, 8401, 7628, 9217, 2053, 7670,\n",
              "        781, 5128, 8316, 5948, 9636, 6788, 7670], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qLVO2ACWbGo",
        "colab_type": "text"
      },
      "source": [
        "## Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUzVpKtMQ2xA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ad3e23aa-3213-41dd-91f9-57b4cfb75b5b"
      },
      "source": [
        "emb_vector_features=40\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,emb_vector_features,input_length=s_len))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 524, 40)           400000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 524, 40)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               56400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 456,501\n",
            "Trainable params: 456,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlqQ8JaLRyct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7bd5f03-eb4e-491e-b5c1-d121700e45a4"
      },
      "source": [
        "len(embedding_docs),y.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18285, (18285,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx2EqLFKS3mZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "x_final=np.array(embedding_docs)\n",
        "y_final=np.array(y)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqijgFgnTGfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "102e71a1-b3a1-4427-d51b-0955a7b030bf"
      },
      "source": [
        "x_final.shape,y_final.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18285, 524), (18285,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NUs7M4DTKh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x_final,y_final,test_size=0.33,random_state=33)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBB4rbNQWLQP",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ISo_OuRTeGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "11d83aa6-e65c-411f-f00a-a6307b692281"
      },
      "source": [
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10,batch_size=64)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "192/192 [==============================] - 8s 40ms/step - loss: 0.4128 - accuracy: 0.8049 - val_loss: 0.2349 - val_accuracy: 0.9148\n",
            "Epoch 2/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.1528 - accuracy: 0.9469 - val_loss: 0.2018 - val_accuracy: 0.9264\n",
            "Epoch 3/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.0883 - accuracy: 0.9709 - val_loss: 0.2146 - val_accuracy: 0.9302\n",
            "Epoch 4/10\n",
            "192/192 [==============================] - 8s 40ms/step - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.2363 - val_accuracy: 0.9284\n",
            "Epoch 5/10\n",
            "192/192 [==============================] - 8s 40ms/step - loss: 0.0426 - accuracy: 0.9884 - val_loss: 0.2455 - val_accuracy: 0.9273\n",
            "Epoch 6/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.3321 - val_accuracy: 0.8689\n",
            "Epoch 7/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.0586 - accuracy: 0.9812 - val_loss: 0.2956 - val_accuracy: 0.9183\n",
            "Epoch 8/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.3230 - val_accuracy: 0.9223\n",
            "Epoch 9/10\n",
            "192/192 [==============================] - 7s 39ms/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.3857 - val_accuracy: 0.9163\n",
            "Epoch 10/10\n",
            "192/192 [==============================] - 8s 39ms/step - loss: 0.0137 - accuracy: 0.9963 - val_loss: 0.3702 - val_accuracy: 0.9215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f958a5bb208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyoZ7Rv8WFVd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZld62HJTqHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5bee0d5-02ed-4994-ee6f-153af95f4f55"
      },
      "source": [
        "y_pred=model.predict_classes(x_test)\n",
        "y_pred[0]"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBUG9TZUHZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a09e91a-abd4-4937-bbbd-52ef7ae3e99f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test,y_pred)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3217,  245],\n",
              "       [ 229, 2344]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtBTfrJUUddW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9af767f5-c066-48cd-86db-8a6c47a86222"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)*100"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92.14581607290803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}